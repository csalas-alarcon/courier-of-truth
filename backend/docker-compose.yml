version: "3.9"

services:
  fastapi:
    build: ./server
    container_name: fastapi_server
    ports:
      - "8000:8000"
    environment:
      LLAMA_SERVER_URL: "http://llama_server:8080"
    depends_on:
      - llama_server
    networks:
      - backend_net

  llama_server:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama_server
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    command: ["--models-dir", "/models"]
    networks:
      - backend_net

networks:
  backend_net:
    driver: bridge
